{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS Static & Realtime parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Looping through all directories within _directory_ of (GTFS) static network data to;\n",
    "* create aggregated dictionary with all types of _trips_ in the public transport network (total 10737 in our case)\n",
    "* create aggregated dictionary with all active _stops_ in the public transport network (total 12854 in our case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "stops = {}\n",
    "ci=13\n",
    "stopsOnTrip={}\n",
    "\n",
    "directory = '../data/static/'\n",
    "\n",
    "for mapp in os.listdir(directory):\n",
    "    print(\"Reading directory\",mapp)\n",
    "    \n",
    "    try:\n",
    "        with open('../data/static/' + mapp + '/stops.txt') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            # Create dictionary key for each row in stops.csv with values on name, latitude and longitude\n",
    "            \n",
    "            for row in reader:\n",
    "                if (row['stop_id'][0:ci] not in stops):\n",
    "                    print(\"\\t\",\"Stop\",row['stop_id'][0:ci],\"added to stops dictionary\")\n",
    "                    stops[row['stop_id'][0:ci]] = {'stop_id': row['stop_id'][0:ci], 'stop_name': row['stop_name'],\n",
    "                                             'stop_lat': row['stop_lat'], 'stop_lon': row['stop_lon']}\n",
    "\n",
    "        with open('../data/static/' + mapp + '/stop_times.txt') as file:\n",
    "            reader1 = csv.DictReader(file)\n",
    "            # Loops through stop_times.csv and creates dictionary with stops per unique trip\n",
    "            \n",
    "            for row in reader1:\n",
    "                \n",
    "                if row['trip_id'] in stopsOnTrip and int(row['stop_sequence']) > len(stopsOnTrip[row['trip_id']]):\n",
    "                    stopsOnTrip[row['trip_id']]\n",
    "                    stopsOnTrip.setdefault(row['trip_id'],[]).append({'stop_sequence':row['stop_sequence'],'stop_id':row['stop_id'][0:ci],'arr_time':row['arrival_time'],'dep_time':row['departure_time']})\n",
    "                    print(\"\\t\",\"Stop\",row['stop_id'][0:ci],\"added to trip\",row['trip_id'])\n",
    "                \n",
    "                elif row['trip_id'] not in stopsOnTrip:\n",
    "                    stopsOnTrip.setdefault(row['trip_id'],[]).append({'stop_sequence':row['stop_sequence'],'stop_id':row['stop_id'][0:ci],'arr_time':row['arrival_time'],'dep_time':row['departure_time']})\n",
    "                    print(\"\\n\",\"Key\",row['trip_id'],\"added to stopsOnTrip dictionary\")\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    except NotADirectoryError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import gzip\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "directory = '../data/tripu/'\n",
    "hms = '%H:%M:%S'\n",
    "\n",
    "# Loop through all folders (days) in directory\n",
    "for folder in os.listdir(directory):\n",
    "    print(\"Reading directory\",folder)\n",
    "    datapoints = []\n",
    "    read_trips = set()\n",
    "\n",
    "    # Loop through all files (requests) per folder (day)\n",
    "    try:\n",
    "        for filename in os.listdir(directory + folder)[::15]:\n",
    "            try:\n",
    "                # Uncompress and parse protobuff-file using gtfs_realtime_pb2\n",
    "                with gzip.open(directory + folder + \"/\" + filename, 'rb') as file:\n",
    "                    response = file.read()\n",
    "                    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "                    feed.ParseFromString(response)\n",
    "\n",
    "                    print(\"Reading filename: \" + filename, \"(\" + str(len(feed.entity)) + \" entities)\")\n",
    "\n",
    "                    for trip in feed.entity:\n",
    "                        this_trip_update = trip.trip_update\n",
    "                        this_trip_id = this_trip_update.trip.trip_id\n",
    "                        print('\\t','Reading trip',this_trip_id)\n",
    "                        if trip.trip_update.trip.trip_id not in read_trips:\n",
    "\n",
    "                            try:\n",
    "                                # Making sure trip_update is on last stop to avoid duplicate data\n",
    "                                if len(this_trip_update.stop_time_update) == len(stopsOnTrip[this_trip_id]):\n",
    "                                    print('\\t','Adding data from trip',this_trip_id)\n",
    "                                    \n",
    "                                    # Creating two iterators to walk through the list of stop_time_update(s)\n",
    "                                    cur_updates = iter(this_trip_update.stop_time_update)\n",
    "                                    nxt_updates = iter(this_trip_update.stop_time_update)\n",
    "                                    \n",
    "                                    # Advance the nxt_updates iterator so it is one ahead of cur_updates\n",
    "                                    next(nxt_updates)\n",
    "\n",
    "                                    for cur_update, nxt_update in zip(cur_updates, nxt_updates):\n",
    "                                        cur_stop_metadata = stops[cur_update.stop_id[0:13]]\n",
    "                                        nxt_stop_metadata = stops[nxt_update.stop_id[0:13]]\n",
    "\n",
    "                                        # Store the delay data point (arrival difference of two ascending nodes)\n",
    "                                        delay = nxt_update.arrival.delay - cur_update.arrival.delay\n",
    "                                        \n",
    "                                        # Convert and store the scheduled duration between current and next stop\n",
    "                                        arrTimeNxt = stopsOnTrip[this_trip_id][nxt_update.stop_sequence-1]['arr_time']\n",
    "                                        arrTimeCur = stopsOnTrip[this_trip_id][cur_update.stop_sequence-1]['arr_time']                                        \n",
    "                                        if int(arrTimeNxt[0:2]) >= 24:\n",
    "                                            arrTimeNxt = '00' + arrTimeNxt[2:8]\n",
    "                                        if int(arrTimeCur[0:2]) >= 24:\n",
    "                                            arrTimeCur = '00' + arrTimeCur[2:8]\n",
    "                                        scheduled_time = (datetime.strptime(arrTimeNxt, hms) - datetime.strptime(arrTimeCur, hms)).seconds\n",
    "\n",
    "                                        # Store contextual metadata\n",
    "                                        date = int(nxt_update.arrival.time)\n",
    "                                        ts = int(nxt_update.arrival.time)\n",
    "                                        key = \"{}/{}\".format(cur_update.stop_id[0:13], nxt_update.stop_id[0:13])\n",
    "                                        \n",
    "                                        source_name=cur_stop_metadata['stop_name']\n",
    "                                        source_id=cur_stop_metadata['stop_id']\n",
    "                                        source_lon=cur_stop_metadata['stop_lon']\n",
    "                                        source_lat=cur_stop_metadata['stop_lat']\n",
    "                                        \n",
    "                                        sink_name=nxt_stop_metadata['stop_name']\n",
    "                                        sink_id=nxt_stop_metadata['stop_id']\n",
    "                                        sink_lon=nxt_stop_metadata['stop_lon']\n",
    "                                        sink_lat=nxt_stop_metadata['stop_lat']\n",
    "                                        \n",
    "                                        # Append data point to array\n",
    "                                        datapoints.append((key,ts,source_name,sink_name,delay,source_lon,source_lat,sink_lon,sink_lat,source_id,sink_id,scheduled_time))\n",
    "\n",
    "                                    read_trips.add(trip.trip_update.trip.trip_id)\n",
    "                                    print(\"\\t\",\"Storing delays for trip_id\",this_trip_id,\"with\",len(this_trip_update.stop_time_update),\"out of\",len(stopsOnTrip[this_trip_id]),\"total stops\")\n",
    "\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        else:\n",
    "                            continue\n",
    "            except OSError:\n",
    "                continue\n",
    "    except NotADirectoryError:\n",
    "        continue\n",
    "\n",
    "    print(\"==========================================\")\n",
    "    print(\"Creating np.array from data in directory\",folder,\"...\")\n",
    "    npdata = np.array(datapoints, dtype=str)\n",
    "\n",
    "    print(\"Creating pandas dataframe with\",len(npdata[:,0]),\"rows ...\")\n",
    "    Edge_ID_col = npdata[:,0]\n",
    "    Unix_TS_col = npdata[:,1]\n",
    "    Date_col = pd.to_datetime(npdata[:,1], unit='s', origin='unix').date\n",
    "    Time_col = pd.to_datetime(npdata[:,1], unit='s', origin='unix').time\n",
    "    Delay_col = pd.to_numeric(npdata[:,4])\n",
    "    Source_col = npdata[:,2]\n",
    "    Sink_col = npdata[:,3]\n",
    "    Source_lon = npdata[:,5]\n",
    "    Source_lat = npdata[:,6]\n",
    "    Sink_lon = npdata[:,7]\n",
    "    Sink_lat = npdata[:,8]\n",
    "    Source_id = npdata[:,9]\n",
    "    Sink_id = npdata[:,10]\n",
    "    Scheduled_duration = pd.to_numeric(npdata[:,11])\n",
    "\n",
    "    d = {'edge_id': Edge_ID_col, \n",
    "         'source_id': Source_id, \n",
    "         'source_name': Source_col, \n",
    "         'source_lon': Source_lon, \n",
    "         'source_lat': Source_lat, \n",
    "         'sink_id': Sink_id, \n",
    "         'sink_name': Sink_col, \n",
    "         'sink_lon': Sink_lon, \n",
    "         'sink_lat': Sink_lat, \n",
    "         'UNIX_TS': Unix_TS_col, \n",
    "         'TIME': Time_col, \n",
    "         'DATE': Date_col,\n",
    "         'scheduled_duration': Scheduled_duration,\n",
    "         'delay': Delay_col}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    print(\"Creating \" + folder + \".csv\")\n",
    "    df.to_csv('../data/csv/' + (str(folder) + '.csv'), encoding='utf-8', index=False)\n",
    "    print(\"Done!\")\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
